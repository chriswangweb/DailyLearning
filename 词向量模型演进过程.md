## 词向量模型演进过程

### 从word2vec、glove、ELMo到BERT
 词向量技术将自然语言中的词转化为稠密的向量，相似的词会有相似的向量表示，这样的转化方便挖掘文字中词语和句子之间的特征。生成词向量的方法从一开始基于统计学的方法（共现矩阵、SVD分解）到基于不同结构的神经网络的语言模型方法。这里总结一下比较经典的语言模型方法：word2vec、glove、ELMo、BERT。
 
 其中BERT是最新Google发表的模型，在11个经典的NLP任务中全面超越最佳模型，并且为下游任务设计了简单至极的接口，改变了之前花销的Attention、Stack等盖楼似的堆叠结构的玩法，应该属于NLP领域里程碑式的贡献。

#### word2vec
word2vec来源于2013年的论文《Efficient Estimation of Word Representation in Vector Space》，它的核心思想是通过词的上下文得到词的向量化表示，有两种方法：CBOW（通过附近词预测中心词）、Skip-gram（通过中心词预测附近的词）

#### glove
word2vec只考虑到了词的局部信息，没有考虑到词与局部窗口外词的联系，glove利用共现矩阵，同时考虑了局部信息和整体的信息。来自论文《Glove: Global vectors for word representation》。

#### ELMo
word2vec和glove存在一个问题，词在不同的语境下其实有不同的含义，而这两个模型词在不同语境下的向量表示是相同的，

ELMo来自于论文《Deep contextualized word representations》，它的官网有开源的工具：https://allennlp.org/elmo

Elmo就是针对这一点进行了优化，作者认为ELMo有两个优势：

1、能够学习到单词用法的复杂特性

2、学习到这些复杂用法在不同上下文的变化

针对点1，作者是通过多层的stack LSTM去学习词的复杂用法，论文中的实验验证了作者的想法，不同层的output可以获得不同层次的词法特征。

针对点2，作者通过pre-train+fine tuning的方式实现，先在大语料库上进行pre-train，再在下游任务的语料库上进行fine tuning。


#### BERT
BERT的工作方式跟ELMo是类似的，但是ELMo存在一个问题，它的语言模型使用的是LSTM，而不是google在2017最新推出的Transformer（来自论文《Attention is all you need》）。LSTM这类序列模型最主要的问题有两个，一是它单方向的，即使是BiLSTM双向模型，也只是在loss处做一个简单的相加，也就是说它是按顺序做推理的，没办法考虑另一个方向的数据；二是它是序列模型，要等前一步计算结束才可以计算下一步，并行计算的能力很差。

所以在ELMo之后，一个新模型GPT（来自论文《Improving Language Understanding by Generative Pre-Training》）推出了，它用Transformer来编码。但是它的推理方式跟ELMo相似，用前面的词去预测下一个词，所以它是单方向，损失掉了下文的信息。

然后BERT诞生了，它采用了Transformer进行编码，预测词的时候双向综合的考虑上下文特征。这里作者做了一个改进，原来没办法综合利用双向的特征是因为目标是用前面的词逐个的预测下一个词，如果你能看到另一个方向，那就等于偷看了答案。BERT的作者说受到了完形填空的启发，遮盖住文章中15%的词，用剩下85%的词预测这15%的词，那就可以放心的利用双向上下文的特征了。

BERT，它来自于googole发表的论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》，这个模型彻底的改变了NLP的游戏规则。
改变游戏规则：

NLP一共有4大类的任务：
- 序列标注：分词／词性标注／命名实体识别…
- 分类任务：文本分类／情感分析…
- 句子关系判断：自然语言推理／深度文本匹配／问答系统…
- 生成式任务：机器翻译／文本摘要生成…

BERT为这4大类任务的前3个都设计了简单至极的下游接口，省去了各种花哨的Attention、stack等复杂的网络结构，且实验效果全面取得了大幅度的提升。
##### Transformer
Transformer是论文《Attention is all you need》中的模型，它把attention机制从配角搬上了主角的位置，没有采用CNN或者RNN的结构，完全使用attention进行编码。Transformer应该会取代CNN和RNN成为NLP主流的编码方式，CNN提取的是局部特征，但是对于文本数据，忽略了长距离的依赖，CNN在文本中的编码能力弱于RNN，而RNN是序列模型，并行能力差，计算缓慢，而且只能考虑一个方向上的信息。Transformer可以综合的考虑两个方向的信息，而且有非常好的并行性质，简单介绍一下Transformer的结构。
参考资料：https://jalammar.github.io/illustrated-transformer/

##### BERT的实现方式：

Mask Language Model
受到完形填空的启发，它不同于传统的语言模型，它是盖住整篇文章15%的词，然后用其他的词预测这15%的词。被盖住的词用[mask]这样的一个标记代替，但是由于下游任务中没有[mask]这个符号，为了削弱这个符号的影响，15%被盖住的词中：
- 80%的词就用[mask]符号盖住
- 10%的词保留原来真实的词
- 10%的词用随机的一个词替代

--------------
